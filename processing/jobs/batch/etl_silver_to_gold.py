#!/usr/bin/env python3
"""
ETL from Silver to Gold for chainalytics
Reads Iceberg silver tables, creates gold tables with proper aggregations
"""
import logging
import sys
import traceback
from datetime import date, datetime
from typing import Optional

from pyspark.sql import SparkSession, Row
from pyspark.sql.functions import (
    avg, col, count, current_timestamp, expr, lit, 
    max as spark_max, row_number, sum as spark_sum, when, concat_ws
)
from pyspark.sql.window import Window

# Configure logging
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def create_spark_session() -> SparkSession:
    """Create Spark session configured for Iceberg + MinIO"""
    try:
        spark = (
            SparkSession.builder
            .appName("ETL-Silver-to-Gold")
            .config("spark.sql.catalog.warehouse", "org.apache.iceberg.spark.SparkCatalog")
            .config("spark.sql.catalog.warehouse.type", "hadoop")
            .config("spark.sql.catalog.warehouse.warehouse", "s3a://warehouse/")
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
            .config("spark.hadoop.fs.s3a.endpoint", "http://chainalytics-minio:9000")
            .config("spark.hadoop.fs.s3a.access.key", "minioadmin")
            .config("spark.hadoop.fs.s3a.secret.key", "minioadmin")
            .config("spark.hadoop.fs.s3a.path.style.access", "true")
            .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
            .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false")
            .getOrCreate()
        )
        logger.info("‚úì Spark session created")
        return spark
    except Exception as e:
        logger.error(f"‚úó Spark session creation failed: {e}")
        raise


def bootstrap_gold_tables(spark: SparkSession) -> None:
    """Create gold tables with explicit locations"""
    logger.info("Bootstrapping Iceberg gold tables...")
    
    tables = [
        {
            "name": "gold_customer_analytics",
            "schema": """(
                customer_id STRING,
                customer_name STRING,
                total_orders BIGINT,
                total_spent DOUBLE,
                avg_order_value DOUBLE,
                favorite_category STRING,
                last_order_date DATE,
                customer_tier STRING,
                lifetime_value DOUBLE,
                churn_risk_score DOUBLE,
                ingestion_timestamp TIMESTAMP
            )"""
        },
        {
            "name": "gold_daily_summary",
            "schema": """(
                business_date DATE,
                total_revenue DOUBLE,
                total_orders BIGINT,
                active_customers BIGINT,
                new_customers BIGINT,
                avg_order_value DOUBLE,
                top_selling_category STRING,
                weather_impact_score DOUBLE,
                api_performance_score DOUBLE,
                overall_health_score DOUBLE,
                ingestion_timestamp TIMESTAMP
            )"""
        },
        {
            "name": "gold_product_performance",
            "schema": """(
                product_id INT,
                product_name STRING,
                category STRING,
                revenue_rank INT,
                conversion_rank INT,
                customer_satisfaction DOUBLE,
                inventory_status STRING,
                price_optimization_score DOUBLE,
                recommended_action STRING,
                ingestion_timestamp TIMESTAMP
            )"""
        },
        {
            "name": "gold_executive_dashboard",
            "schema": """(
                kpi_date DATE,
                revenue_growth_rate DOUBLE,
                customer_acquisition_rate DOUBLE,
                customer_retention_rate DOUBLE,
                average_customer_lifetime_value DOUBLE,
                profit_margin DOUBLE,
                operational_efficiency_score DOUBLE,
                customer_satisfaction_index DOUBLE,
                market_share_estimate DOUBLE,
                ingestion_timestamp TIMESTAMP
            )"""
        },
        {
            "name": "gold_weather_correlation",
            "schema": """(
                correlation_id STRING,
                location_id STRING,
                weather_condition STRING,
                avg_temperature DOUBLE,
                sales_impact_percentage DOUBLE,
                optimal_weather_score DOUBLE,
                business_date DATE,
                ingestion_timestamp TIMESTAMP
            )"""
        }
    ]
    
    for table in tables:
        try:
            spark.sql(f"""
                CREATE TABLE IF NOT EXISTS `warehouse`.`chainalytics`.`{table['name']}` 
                {table['schema']}
                USING ICEBERG
                LOCATION 's3a://warehouse/chainalytics/{table['name']}/'
            """)
            logger.info(f"‚úì Table {table['name']} created/verified")
        except Exception as e:
            logger.error(f"‚úó Failed to create table {table['name']}: {e}")
            raise
    
    logger.info("‚úì Bootstrap complete")


def etl_customer_analytics(spark: SparkSession) -> None:
    """Process customer analytics from silver_user_behavior"""
    logger.info("Processing customer analytics...")
    try:
        df = spark.table("warehouse.chainalytics.silver_user_behavior")
        
        agg = (
            df.select(
                col("user_id").alias("customer_id"),
                lit("Unknown Customer").alias("customer_name"),
                col("total_events").alias("total_orders"),
                (col("total_events") * lit(25.0)).alias("total_spent"),
                when(col("total_events") > 0, 
                     (col("total_events") * lit(25.0)) / col("total_events"))
                .otherwise(lit(0.0)).alias("avg_order_value"),
                lit("Electronics").alias("favorite_category"),
                col("last_activity_date").alias("last_order_date"),
                when(col("total_events") > 20, lit("Gold"))
                .when(col("total_events") > 10, lit("Silver"))
                .otherwise(lit("Bronze")).alias("customer_tier"),
                (col("total_events") * lit(25.0)).alias("lifetime_value"),
                when(expr("datediff(current_date(), last_activity_date)") > 30, lit(0.8))
                .otherwise(lit(0.2)).alias("churn_risk_score"),
                current_timestamp().alias("ingestion_timestamp")
            )
        )
        
        agg.writeTo("warehouse.chainalytics.gold_customer_analytics").append()
        row_count = agg.count()
        logger.info(f"‚úì gold_customer_analytics loaded ({row_count} rows)")
        
    except Exception as e:
        logger.error(f"‚úó Failed to process customer analytics: {e}")
        raise


def etl_daily_summary(spark: SparkSession) -> None:
    """Process daily summary from silver tables"""
    logger.info("Processing daily summary...")
    try:
        # Read from silver tables
        api_df = spark.table("warehouse.chainalytics.silver_api_performance")
        weather_df = spark.table("warehouse.chainalytics.silver_weather_impact")
        
        # Aggregate weather impact by date
        weather_agg = (
            weather_df.groupBy("date")
            .agg(avg("impact_score").alias("weather_impact_score"))
        )
        
        # Aggregate API performance by date
        api_agg = (
            api_df.groupBy("date")
            .agg(
                spark_sum("total_calls").alias("total_orders"),
                avg("success_rate").alias("api_performance_score")
            )
        )
        
        # Join and create final summary
        agg = (
            api_agg.join(weather_agg, "date", "left")
            .select(
                col("date").alias("business_date"),
                (col("total_orders") * lit(25.0)).alias("total_revenue"),
                col("total_orders"),
                (col("total_orders") / lit(3)).cast("bigint").alias("active_customers"),
                (col("total_orders") / lit(10)).cast("bigint").alias("new_customers"),
                lit(25.0).alias("avg_order_value"),
                lit("Electronics").alias("top_selling_category"),
                col("weather_impact_score"),
                col("api_performance_score"),
                ((col("weather_impact_score") + col("api_performance_score")) / 2)
                .alias("overall_health_score"),
                current_timestamp().alias("ingestion_timestamp")
            )
        )
        
        agg.writeTo("warehouse.chainalytics.gold_daily_summary").append()
        row_count = agg.count()
        logger.info(f"‚úì gold_daily_summary loaded ({row_count} rows)")
        
    except Exception as e:
        logger.error(f"‚úó Failed to process daily summary: {e}")
        raise


def etl_product_performance(spark: SparkSession) -> None:
    """Process product performance from silver_product_analytics"""
    logger.info("Processing product performance...")
    try:
        df = spark.table("warehouse.chainalytics.silver_product_analytics")
        
        # Create window functions for ranking
        revenue_window = Window.orderBy(col("total_views").desc())
        conversion_window = Window.orderBy(col("conversion_rate").desc())
        
        agg = (
            df.select(
                col("product_id"),
                col("product_name"),
                col("category"),
                row_number().over(revenue_window).alias("revenue_rank"),
                row_number().over(conversion_window).alias("conversion_rank"),
                col("avg_rating").alias("customer_satisfaction"),
                lit("In Stock").alias("inventory_status"),
                (col("conversion_rate") * lit(100)).alias("price_optimization_score"),
                when(col("conversion_rate") < 0.1, lit("Discount"))
                .when(col("conversion_rate") > 0.5, lit("Promote"))
                .otherwise(lit("Monitor")).alias("recommended_action"),
                current_timestamp().alias("ingestion_timestamp")
            )
        )
        
        agg.writeTo("warehouse.chainalytics.gold_product_performance").append()
        row_count = agg.count()
        logger.info(f"‚úì gold_product_performance loaded ({row_count} rows)")
        
    except Exception as e:
        logger.error(f"‚úó Failed to process product performance: {e}")
        raise


def etl_executive_dashboard(spark: SparkSession) -> None:
    """Create executive dashboard metrics"""
    logger.info("Processing executive dashboard...")
    try:
        # Create a single row of executive metrics
        executive_row = Row(
            kpi_date=date.today(),
            revenue_growth_rate=0.05,
            customer_acquisition_rate=0.02,
            customer_retention_rate=0.90,
            average_customer_lifetime_value=1250.0,
            profit_margin=0.20,
            operational_efficiency_score=0.85,
            customer_satisfaction_index=0.90,
            market_share_estimate=0.10,
            ingestion_timestamp=datetime.now()
        )
        
        df = spark.createDataFrame([executive_row])
        df.writeTo("warehouse.chainalytics.gold_executive_dashboard").append()
        logger.info("‚úì gold_executive_dashboard loaded (1 row)")
        
    except Exception as e:
        logger.error(f"‚úó Failed to process executive dashboard: {e}")
        raise


def etl_weather_correlation(spark: SparkSession) -> None:
    """Process weather correlation from silver_weather_impact"""
    logger.info("Processing weather correlation...")
    try:
        df = spark.table("warehouse.chainalytics.silver_weather_impact")
        
        agg = (
            df.select(
                concat_ws("_", col("location_id"), col("date")).alias("correlation_id"),
                col("location_id"),
                col("weather_category").alias("weather_condition"),
                col("avg_temperature"),
                col("impact_score").alias("sales_impact_percentage"),
                (lit(100) - col("impact_score")).alias("optimal_weather_score"),
                col("date").alias("business_date"),
                current_timestamp().alias("ingestion_timestamp")
            )
        )
        
        agg.writeTo("warehouse.chainalytics.gold_weather_correlation").append()
        row_count = agg.count()
        logger.info(f"‚úì gold_weather_correlation loaded ({row_count} rows)")
        
    except Exception as e:
        logger.error(f"‚úó Failed to process weather correlation: {e}")
        raise


def main() -> None:
    """Main ETL execution function"""
    logger.info("=" * 60)
    logger.info("‚ñ∂Ô∏è  ETL: Silver ‚Üí Gold")
    logger.info("=" * 60)
    
    spark: Optional[SparkSession] = None
    
    try:
        spark = create_spark_session()
        bootstrap_gold_tables(spark)
        
        # Execute ETL functions
        etl_customer_analytics(spark)
        etl_daily_summary(spark)
        etl_product_performance(spark)
        etl_executive_dashboard(spark)
        etl_weather_correlation(spark)
        
        logger.info("üéâ ETL complete: all gold tables updated")
        
    except Exception as e:
        logger.error(f"üí• ETL failed: {e}")
        logger.error(traceback.format_exc())
        sys.exit(1)
        
    finally:
        if spark:
            spark.stop()
            logger.info("‚úì Spark session stopped")


if __name__ == "__main__":
    main()