
services:
  minio:
    image: minio/minio:latest
    container_name: chainalytics-minio
    ports:
      - "9000:9000"   # S3 API
      - "9001:9001"   # MinIO Console
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - ../data/minio:/data
    networks:
      - chainalytics-network

  spark-master:
    user: root

    build:
      context: .
      dockerfile: Dockerfile
    image: chainalytics/spark-master:3.5.0
    container_name: chainalytics-spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_MASTER_OPTS=
      - SPARK_DAEMON_MEMORY=1g

    ports:
      - "8081:8080"  # Spark Master Web UI
      - "7077:7077"  # Spark Master Port
      - "4040:4040"  # Spark Application UI
    volumes:
      - ./jobs:/opt/spark-jobs
      - ./config/spark-defaults.conf:/opt/bitnami/spark/config/spark-defaults.conf:ro
      - spark-logs:/opt/bitnami/spark/logs
      - spark-events:/tmp/spark-events
      - ../data/minio:/data/minio  # << Add this line to mount your CSV folder
      - ./processing/jobs:/opt/processing-jobs   # <-- add this line


    networks:
      - chainalytics-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/"]
      interval: 10s
      timeout: 5s
      retries: 5

  spark-worker-1:
    build:
      context: .
      dockerfile: Dockerfile
    image: chainalytics/spark-worker-1:3.5.0
    container_name: chainalytics-spark-worker-1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_WORKER_OPTS=-Xmx2g -XX:MaxPermSize=256m
    depends_on:
      spark-master:
        condition: service_healthy
    volumes:
      - ./jobs:/opt/spark-jobs
      - ./jars:/opt/bitnami/spark/jars
      - ./config/spark-defaults.conf:/opt/bitnami/spark/config/spark-defaults.conf:ro
      - spark-logs:/opt/bitnami/spark/logs
      - spark-events:/tmp/spark-events
      - ../data/minio:/data/minio  # << Add this line to mount your CSV folder

    networks:
      - chainalytics-network

  spark-worker-2:
    build:
      context: .
      dockerfile: Dockerfile
    image: chainalytics/spark-worker-2:3.5.0
    container_name: chainalytics-spark-worker-2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_WORKER_OPTS=-Xmx2g -XX:MaxPermSize=256m
    depends_on:
      spark-master:
        condition: service_healthy
    volumes:
      - ./jobs:/opt/spark-jobs
      - ./jars:/opt/bitnami/spark/jars
      - ./config/spark-defaults.conf:/opt/bitnami/spark/config/spark-defaults.conf:ro
      - spark-logs:/opt/bitnami/spark/logs
      - spark-events:/tmp/spark-events
      - ../data/minio:/data/minio  # << Add this line to mount your CSV folder

    networks:
      - chainalytics-network

  # Spark History Server for monitoring completed jobs
  spark-history:
    build:
      context: .
      dockerfile: Dockerfile
    image: chainalytics/spark-history:3.5.0
    container_name: chainalytics-spark-history
    environment:
      - SPARK_MODE=history-server
      - SPARK_HISTORY_OPTS=-Dspark.history.fs.logDirectory=/tmp/spark-events
    ports:
      - "18080:18080"
    volumes:
      - spark-events:/tmp/spark-events
      - ./config/spark-defaults.conf:/opt/bitnami/spark/config/spark-defaults.conf:ro
    networks:
      - chainalytics-network
    depends_on:
      - spark-master

  # Thrift Server for SQL queries (optional but useful for debugging)
  spark-thrift:
    build:
      context: .
      dockerfile: Dockerfile
    image: chainalytics/spark-thrift:3.5.0
    container_name: chainalytics-spark-thrift
    environment:
      - SPARK_MODE=master  # Thrift server runs in master mode
      - SPARK_MASTER_URL=spark://spark-master:7077
    command: |
      /bin/bash -c "
        /opt/bitnami/spark/sbin/start-thriftserver.sh \
          --master spark://spark-master:7077 \
          --conf spark.sql.warehouse.dir=s3a://warehouse/ \
          --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000 \
          --conf spark.hadoop.fs.s3a.access.key=minioadmin \
          --conf spark.hadoop.fs.s3a.secret.key=minioadmin \
          --conf spark.hadoop.fs.s3a.path.style.access=true \
          --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \
          --hiveconf hive.server2.thrift.port=10000 \
          --hiveconf hive.server2.thrift.bind.host=0.0.0.0
      "
    ports:
      - "10000:10000"  # Thrift server port
      - "4041:4040"    # Spark UI for Thrift server
    volumes:
      - ./jars:/opt/bitnami/spark/jars
      - ./config/spark-defaults.conf:/opt/bitnami/spark/config/spark-defaults.conf:ro
    networks:
      - chainalytics-network
    depends_on:
      spark-master:
        condition: service_healthy

volumes:
  spark-logs:
  spark-events:

networks:
  chainalytics-network:
    external: true
